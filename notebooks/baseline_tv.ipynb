{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3esu72VBoQW9"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers seqeval evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or_CtBwApFRU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from src import utils\n",
    "from src import pre_processing\n",
    "from transformers import create_optimizer\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxE8ny2XpJmb"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples: dict):\n",
    "    \"\"\"Tokenize and align labels with subword tokens.\n",
    "\n",
    "    Args:\n",
    "        examples: Pre-token.\n",
    "\n",
    "    Returns:\n",
    "        Tokens with labels.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    all_labels = examples['aspect_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(utils.align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1670505982311,
     "user": {
      "displayName": "José Carlos Ferreira Neto",
      "userId": "09993765487513772420"
     },
     "user_tz": 180
    },
    "id": "QSye0Ev2pJrd",
    "outputId": "6010f62a-8aa6-49bc-ad43-4e815b7e60fd"
   },
   "outputs": [],
   "source": [
    "# pre-processing the data\n",
    "data_ds = pre_processing.pre_processing_tv_dataset('datasets/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W0NrSN7pYNT"
   },
   "outputs": [],
   "source": [
    "# tag mapping\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label_names = ['O', 'B-ASP', 'I-ASP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cwSIvo0pYPz"
   },
   "outputs": [],
   "source": [
    "# pre-trained models\n",
    "models = [\n",
    "    'neuralmind/bert-base-portuguese-cased',\n",
    "    'neuralmind/bert-large-portuguese-cased',\n",
    "    'bert-base-multilingual-cased'\n",
    "]\n",
    "\n",
    "# fine tuning\n",
    "results = defaultdict(list)\n",
    "for model_checkpoint in models:\n",
    "\n",
    "    # running 5 times\n",
    "    for _ in range(5):\n",
    "\n",
    "        # initializing the tokenizer\n",
    "        tokenizer = utils.build_tokenizer(model_checkpoint)\n",
    "\n",
    "        # tokenizing and aligning\n",
    "        tokenized_dataset = data_ds.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=data_ds.column_names)\n",
    "\n",
    "        # separating into training, testing and validation\n",
    "        data = utils.train_test_val_split(\n",
    "            tokenized_dataset, \n",
    "            test_size=0.1, \n",
    "            val_size=0.1)\n",
    "\n",
    "        # creating data collator\n",
    "        data_collator = DataCollatorForTokenClassification(\n",
    "            tokenizer=tokenizer,\n",
    "            return_tensors='tf')\n",
    "        columns = ['attention_mask', 'input_ids', 'labels', 'token_type_ids']\n",
    "        tf_dataset = utils.dataset_to_tf_dataset(\n",
    "            data=data, \n",
    "            data_collator=data_collator, \n",
    "            columns=columns, \n",
    "            batch_size=8)\n",
    "\n",
    "        # defining the number of epochs and steps\n",
    "        num_epochs = 3\n",
    "        num_train_steps = len(tf_dataset['train']) * num_epochs\n",
    "\n",
    "        # defining the optimizer\n",
    "        optimizer, _ = create_optimizer(\n",
    "            init_lr=2e-5,\n",
    "            num_warmup_steps=0,\n",
    "            num_train_steps=num_train_steps,\n",
    "            weight_decay_rate=0.01,)\n",
    "\n",
    "        # defining the model\n",
    "        model = utils.build_model(\n",
    "            model_checkpoint=model_checkpoint, \n",
    "            id2label=id2label, \n",
    "            label2id=label2id, \n",
    "            from_pt=True)\n",
    "        model.compile(optimizer=optimizer)\n",
    "\n",
    "        # training the model\n",
    "        model.fit(\n",
    "            tf_dataset['train'],\n",
    "            validation_data=tf_dataset['validation'],\n",
    "            epochs=num_epochs)\n",
    "\n",
    "        # evaluating the model\n",
    "        result = utils.evaluate_model(\n",
    "            model=model,\n",
    "            test_data=tf_dataset['test'],\n",
    "            label_names=label_names)\n",
    "        results[model_checkpoint].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNN_uHbiptZl"
   },
   "outputs": [],
   "source": [
    "# extracting the results\n",
    "final_results = defaultdict(lambda: defaultdict(list))\n",
    "for model in results.keys():\n",
    "    for r in results[model]:\n",
    "        for metric in ('overall_precision', 'overall_recall', 'overall_f1'):\n",
    "            final_results[model][metric].append(r[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_o7zSsj4IUA"
   },
   "outputs": [],
   "source": [
    "# average results\n",
    "avg_results = defaultdict(lambda: defaultdict(float))\n",
    "for model in final_results.keys():\n",
    "    for metric in ('overall_precision', 'overall_recall', 'overall_f1'):\n",
    "        avg_results[model][metric] = np.mean(final_results[model][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1670455751224,
     "user": {
      "displayName": "José Carlos Ferreira Neto",
      "userId": "09993765487513772420"
     },
     "user_tz": 180
    },
    "id": "pKzuuqBV4jpA",
    "outputId": "f7b83f76-36e2-4f2b-e148-25ed0bfe6aea"
   },
   "outputs": [],
   "source": [
    "# formatando opara melhor visualização\n",
    "avg_results = pd.DataFrame(avg_results)\n",
    "avg_results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNpCvjTVqetbbEyiIdF/u+A",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
